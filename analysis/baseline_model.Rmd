---
title: "A Simple Keras Facial Keypoint Detector"
author: "Miles McBain"
date: 2017-09-19
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->
# Creating a baseline Keras Model

I'm going to build a very basic keras model that can detect a single facial keypoint. Later I will generalise this to many keypoints.

## Source and Wrangle a Training Set

Get the training set:

```{r, results="hide"}
library(readr)
library(keras)
library(dplyr)

train_set <- read_rds("../data/train_processed")
# Original data from: https://www.kaggle.com/c/facial-keypoints-detection
# Preprocessing done in /code/preprocess_kaggle_set.R
train_set <- 
  train_set %>%
  filter(!is.na(left_eye_center_x), !(is.na(left_eye_center_y)))
```

The images are 96x96 256 greyscale, represented in 9216 vector pixels.

Check out an image:
```{r}
image(matrix(rev(train_set$image[[42]]), 96, 96), col = gray.colors(256))
```


Choose X (the image) and Y (a facial keypoint):

```{r, echo=FALSE}
X <- train_set$image
Y_1 <- train_set$left_eye_center_x
Y_2 <- train_set$left_eye_center_y
```

Scale `X` pixel values from 0 - 255 to 0 - 1:

```{r}
X <- unlist(X) / 255
dim(X) <- c(nrow(train_set), 96*96)
```

Scale `Y` values from 0 - 96 to 0 - 1:

```{r}
Y_1 <- Y_1 / 96
Y_2 <- Y_2 / 96 
```

## Create a network model

Define a simple model with [256 -> 128 -> 2] with dropout layers that apparently ensure features are distinct. No convolutions yet.

```{r}
model_input <- layer_input(shape = c(96*96), dtype = 'float32', name = 'image_input')
main_network <- 
  model_input %>%
  layer_dense(units = 256, activation = 'relu', input_shape = c(96*96)) %>%
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 2, activation = 'relu')

output_y1 <- 
  main_network %>%
  layer_dense(units = 1, activation = "linear", name = "output_y1")

output_y2 <-
  main_network %>%
  layer_dense(units = 1, activation = "linear", name = "output_y2")

model <- keras_model(
  inputs = model_input,
  outputs = c(output_y1, output_y2)
)

model 
```


## Compile model

```{r}
model %>%
  compile(loss = 'mean_squared_error', # MSE for continuous output
          loss_weights = c(0.5, 0.5), # Weight both coords equally 
          optimizer = optimizer_adam())
```

## Fit model

```{r}
fit_progress <- 
  model %>%
  fit(x = X,
      y = list(output_y1 = Y_1, output_y2 = Y_2),
      epochs = 30,
      batch_size = 128, 
      validation_split = 0.2)
```

## Visualise Model
```{r}
plot(fit_progress)
```

## Predict my eye
```{r}
miles_face <- 
  read_rds("../data/mm_face_vector.rds")
# A cropped square selfie.
# Preprocessing with magick in /code/preprocess_a_selfie.R
  

miles_face_matrix <-
  miles_face %>%
  `/`(256) %>%
  matrix(nrow = 1, ncol = 96*96 )

eye_prediction <- 
  model %>%
  predict_on_batch(miles_face_matrix)

image(matrix(miles_face %>% rev(), 96, 96) , col = gray.colors(256)) # Have to reverse image for upright viewing
points(x = 1- eye_prediction[[1]], y= 1- eye_prediction[[2]]) # Reversed image -> Take compliement of coord.
```

Not bad for a baseline model. Can I do better with convolutions? Most likely!

## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
